{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-27T12:47:44.593264400Z",
     "start_time": "2024-05-27T12:47:44.498672600Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from models.cnn import CNN\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "from PIL import Image\n",
    "from scipy.ndimage import binary_closing, binary_opening\n",
    "from utils.display import show_image\n",
    "from utils.data_augmentation import pad_tensor_with_borders\n",
    "\n",
    "model = CNN()\n",
    "model_name=\"modelb128e100_more_data_arch.pth\"\n",
    "model.load_state_dict(torch.load(f'models/{model_name}'))\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ground_truth = Image.open('data/sfa/ORI/img (92).jpg')\n",
    "original_width, original_height = ground_truth.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "patch_size = 35\n",
    "half_patch_size = patch_size // 2\n",
    "stride = 1\n",
    "padded_image = pad_tensor_with_borders(Compose([ToTensor()])(ground_truth), original_height + half_patch_size, original_width + half_patch_size)\n",
    "show_image(padded_image)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T12:47:44.789997600Z",
     "start_time": "2024-05-27T12:47:44.563618300Z"
    }
   },
   "id": "269cd8dec903f412"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def get_image_predictions(padded_image):\n",
    "    \n",
    "    unfolded = padded_image.unfold(1, patch_size, stride).unfold(2, patch_size, stride)\n",
    "    unfolded = unfolded.permute(1, 2, 0, 3, 4).contiguous()\n",
    "    patches = unfolded.view(-1, 3, patch_size, patch_size)\n",
    "\n",
    "    num_patches = patches.size(0)\n",
    "    batch_size = 1024\n",
    "    num_batches = int(np.ceil(num_patches / batch_size))\n",
    "    \n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        model.to(device)\n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min(start_idx + batch_size, num_patches)\n",
    "            batch_patches = patches[start_idx:end_idx]\n",
    "            # Move the batch to the same device as the model\n",
    "            batch_patches = batch_patches.to(device)\n",
    "            # Predict using the trained model\n",
    "            predictions = model(batch_patches)\n",
    "            # Move predictions to CPU and convert to numpy\n",
    "            predictions = predictions.cpu().numpy()\n",
    "            \n",
    "            all_predictions.append(predictions)\n",
    "    \n",
    "    # Concatenate all predictions into a single array\n",
    "    all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "    \n",
    "    return all_predictions.reshape(original_height, original_width)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T12:47:44.801180600Z",
     "start_time": "2024-05-27T12:47:44.798331100Z"
    }
   },
   "id": "7268fe754146c62b"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def draw_mask(image_predictions, display=False):\n",
    "    new_image = Image.new('L', (original_width, original_height), 0)\n",
    "    # Iterate over each pixel in the new image and set it based on predictions\n",
    "    for y in range(original_height):\n",
    "        for x in range(original_width):\n",
    "            prediction = image_predictions[y, x]\n",
    "            new_pixel_value = 255 if prediction >= 0.5 else 0\n",
    "            new_image.putpixel((x, y), new_pixel_value)\n",
    "    \n",
    "    if display:\n",
    "        new_image.show(\"predicted_image.jpg\")\n",
    "        \n",
    "    return new_image"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T12:47:44.818665400Z",
     "start_time": "2024-05-27T12:47:44.801180600Z"
    }
   },
   "id": "4d36532513a180d9"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "preds = get_image_predictions(padded_image)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T12:48:01.982017100Z",
     "start_time": "2024-05-27T12:47:44.814149700Z"
    }
   },
   "id": "cd061fad700de079"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "mask = draw_mask(preds, False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T12:48:03.040946200Z",
     "start_time": "2024-05-27T12:48:01.982017100Z"
    }
   },
   "id": "d77ccde401edc163"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# Step 3: Smoothen the mask using morphological operations\n",
    "mask_np = np.array(mask)\n",
    "kernel = np.ones((3, 3), np.uint8)\n",
    "mask_np = binary_closing(mask_np, structure=kernel)\n",
    "mask_np = binary_opening(mask_np, structure=kernel)\n",
    "smoothened_mask = Image.fromarray((mask_np * 255).astype(np.uint8))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T12:48:03.066893200Z",
     "start_time": "2024-05-27T12:48:03.040946200Z"
    }
   },
   "id": "2ffc27d8f9e44d82"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# Step 4: Extract the skin area\n",
    "skin_area = Image.composite(ground_truth, Image.new('RGB', (original_width, original_height)), smoothened_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T12:48:03.077466Z",
     "start_time": "2024-05-27T12:48:03.066893200Z"
    }
   },
   "id": "144576ed437ed7d"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 38128\n",
      "True Negatives: 330909\n",
      "False Positives: 2703\n",
      "False Negatives: 21476\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Evaluation (Example, assuming ground truth is available)\n",
    "ground_truth = Image.open('data/sfa/GT/img (92).jpg').convert('L')\n",
    "ground_truth_np = np.array(ground_truth)\n",
    "smoothened_mask_np = np.array(smoothened_mask)\n",
    "ground_truth_np_skin = (ground_truth_np > 15).astype(int)\n",
    "\n",
    "TP = np.sum((ground_truth_np_skin == 1) & (smoothened_mask_np == 255))\n",
    "TN = np.sum((ground_truth_np_skin == 0) & (smoothened_mask_np == 0))\n",
    "FP = np.sum((ground_truth_np_skin == 0) & (smoothened_mask_np == 255))\n",
    "FN = np.sum((ground_truth_np_skin == 1) & (smoothened_mask_np == 0))\n",
    "\n",
    "print(f\"True Positives: {TP}\")\n",
    "print(f\"True Negatives: {TN}\")\n",
    "print(f\"False Positives: {FP}\")\n",
    "print(f\"False Negatives: {FN}\")\n",
    "\n",
    "# Display results\n",
    "ground_truth.show(title='Original Image')\n",
    "smoothened_mask.show(title='Skin Mask')\n",
    "skin_area.show(title='Extracted Skin Area')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T12:48:03.187033700Z",
     "start_time": "2024-05-27T12:48:03.077466Z"
    }
   },
   "id": "ee786dc5cee06c68"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9385\n",
      "Precision: 0.9338\n",
      "Recall: 0.6397\n",
      "F1 Score: 0.7593\n",
      "MCC: 0.7425\n",
      "IoU: 0.6119\n"
     ]
    }
   ],
   "source": [
    "TP, TN, FP, FN = map(float, [TP, TN, FP, FN])\n",
    "\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "mcc_numerator = (TP * TN) - (FP * FN)\n",
    "mcc_denominator = ((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN)) ** 0.5\n",
    "mcc = mcc_numerator / mcc_denominator if mcc_denominator != 0 else 0\n",
    "\n",
    "iou = TP / (TP + FP + FN)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")\n",
    "print(f\"MCC: {mcc:.4f}\")\n",
    "print(f\"IoU: {iou:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T12:48:03.188500Z",
     "start_time": "2024-05-27T12:48:03.155578700Z"
    }
   },
   "id": "725e9288ae458ad6"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Save metrics to CSV file\n",
    "with open(\"results/evaluation_results.csv\", 'a', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([model_name])\n",
    "    writer.writerow(['Accuracy', 'Precision', 'Recall', 'F1 Score', 'MCC', 'IoU'])\n",
    "    writer.writerow([accuracy, precision, recall, f1_score, mcc, iou])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-27T12:48:03.281633700Z",
     "start_time": "2024-05-27T12:48:03.188500Z"
    }
   },
   "id": "57846cb53a692100"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "skin-detection-kernel",
   "language": "python",
   "display_name": "skin-detection-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
